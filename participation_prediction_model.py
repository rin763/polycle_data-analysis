# pip install -q scikit-learn pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib.font_manager as fm
japanese_fonts = ['Hiragino Kaku Gothic ProN', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao Gothic']
available_fonts = [f.name for f in fm.fontManager.ttflist]

japanese_font = None
for font in japanese_fonts:
    if font in available_fonts:
        japanese_font = font
        break

if japanese_font:
    plt.rcParams['font.family'] = japanese_font
    print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{japanese_font}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
else:
    plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False

class ParticipationPredictionModel:
    """ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, csv_file):
        self.df = self.load_and_preprocess_data(csv_file)
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.feature_names = []
        self.models = {}
        self.feature_importance = {}
        
    def load_and_preprocess_data(self, csv_file):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(csv_file, encoding='shift_jis')
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {df.shape[0]}åã®å‚åŠ è€…")
        
        # åŸºæœ¬å‰å‡¦ç†
        df = df.copy()
        df['å­¦å¹´'] = pd.to_numeric(df['å­¦å¹´'].astype(str).str.replace(r'[^\d]', '', regex=True), errors='coerce')
        
        # å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
        df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = df['å­¦ç§‘'].apply(self.categorize_major)
        
        return df
    
    def categorize_major(self, major_name):
        """å­¦ç§‘åã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        if pd.isna(major_name):
            return 'ãã®ä»–'
        
        name = str(major_name).upper()
        
        if 'æƒ…å ±' in name or 'è³‡è¨Š' in name or 'CS' in name:
            return 'æƒ…å ±'
        elif 'åŒ»å­¦' in name or 'é†«' in name or 'è–¬å­¦' in name:
            return 'åŒ»å­¦'
        elif 'å·¥å­¦' in name or 'å·¥ç¨‹' in name or 'é›»æ©Ÿ' in name:
            return 'å·¥å­¦'
        elif 'ç†å­¦' in name or 'ç§‘å­¸' in name or 'ç‰©ç†' in name:
            return 'ç†å­¦'
        elif 'å»ºç¯‰' in name:
            return 'å»ºç¯‰'
        elif 'è²¿æ˜“' in name or 'åœ‹éš›' in name or 'å›½éš›' in name:
            return 'å›½éš›'
        elif 'è¨€èª' in name or 'èªå­¸' in name or 'å¤–æ–‡' in name or 'è‹±æ–‡' in name:
            return 'äººæ–‡'
        elif 'çµŒæ¸ˆ' in name or 'ä¼ç®¡' in name or 'å•†å­¸' in name or 'ç®¡ç†' in name:
            return 'å•†å­¦'
        elif 'æ•™è‚²' in name or 'å¸«è³‡' in name:
            return 'æ•™è‚²'
        elif 'è¦³å…‰' in name or 'è§€å…‰' in name:
            return 'è¦³å…‰'
        elif 'é‹å‹•' in name:
            return 'é‹å‹•'
        elif 'è¨­è¨ˆ' in name:
            return 'ãƒ‡ã‚¶ã‚¤ãƒ³'
        else:
            return 'ãã®ä»–'
    
    def feature_engineering(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
        
        # åŸºæœ¬ç‰¹å¾´é‡
        features_df = pd.DataFrame()
        
        # 1. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        categorical_features = ['æ€§åˆ¥', 'å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'å­¦æ ¡å', 'é–‹å‚¬åœ°ç‚¹', 'é–‹å‚¬å ´æ‰€ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'èªçŸ¥ãã£ã‹ã‘']
        
        for feature in categorical_features:
            if feature in self.df.columns:
                # æ¬ æå€¤ã‚’'Unknown'ã§åŸ‹ã‚ã‚‹
                self.df[feature] = self.df[feature].fillna('Unknown')
                
                # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                le = LabelEncoder()
                encoded_values = le.fit_transform(self.df[feature].astype(str))
                features_df[f'{feature}_encoded'] = encoded_values
                self.label_encoders[feature] = le
        
        # 2. æ•°å€¤ç‰¹å¾´é‡
        numerical_features = ['å­¦å¹´', 'è¦æ¨¡ï¼ˆäººï¼‰', 'æ–™é‡‘ï¼ˆå…ƒï¼‰']
        for feature in numerical_features:
            if feature in self.df.columns:
                features_df[feature] = pd.to_numeric(self.df[feature], errors='coerce').fillna(0)
        
        # 3. çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰
        if 'å­¦å¹´' in features_df.columns and 'å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼_encoded' in features_df.columns:
            features_df['å­¦å¹´Ã—å­¦ç§‘'] = features_df['å­¦å¹´'] * features_df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼_encoded']
        
        if 'èªçŸ¥ãã£ã‹ã‘_encoded' in features_df.columns and 'å­¦æ ¡å_encoded' in features_df.columns:
            features_df['èªçŸ¥ãã£ã‹ã‘Ã—å­¦æ ¡'] = features_df['èªçŸ¥ãã£ã‹ã‘_encoded'] * features_df['å­¦æ ¡å_encoded']
        
        if 'æ€§åˆ¥_encoded' in features_df.columns and 'å­¦å¹´' in features_df.columns:
            features_df['æ€§åˆ¥Ã—å­¦å¹´'] = features_df['æ€§åˆ¥_encoded'] * features_df['å­¦å¹´']
        
        # 4. æ™‚é–“ç‰¹å¾´é‡
        if 'æ™‚é–“å¸¯' in self.df.columns:
            # æ™‚é–“å¸¯ã‚’æ•°å€¤ã«å¤‰æ›
            time_mapping = {
                'æœ': 1, 'åˆå‰': 2, 'æ˜¼': 3, 'åˆå¾Œ': 4, 'å¤•æ–¹': 5, 'å¤œ': 6
            }
            features_df['æ™‚é–“å¸¯_encoded'] = self.df['æ™‚é–“å¸¯'].map(time_mapping).fillna(0)
        
        # 5. ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆTF-IDFï¼‰
        text_columns = ['æœŸå¾…ã—ã¦ã„ã‚‹ã“ã¨', 'ã©ã‚“ãªæ–¹ã¨ãŠè©±ã—ã—ã¦ã¿ãŸã„ã‹', 'å¾—æ„ãªã“ã¨ï¼æŒ‘æˆ¦ã—ãŸã„ã“ã¨ï¼èˆˆå‘³ã‚ã‚‹ã“ã¨']
        combined_text = []
        
        for idx, row in self.df.iterrows():
            text_parts = []
            for col in text_columns:
                if col in self.df.columns and pd.notna(row[col]):
                    text_parts.append(str(row[col]))
            combined_text.append(' '.join(text_parts))
        
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆä¸Šä½20ç‰¹å¾´é‡ï¼‰
        if any(combined_text):
            tfidf = TfidfVectorizer(max_features=20, stop_words=None)
            tfidf_matrix = tfidf.fit_transform(combined_text)
            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), 
                                  columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])
            features_df = pd.concat([features_df, tfidf_df], axis=1)
        
        # 6. çµ±è¨ˆç‰¹å¾´é‡
        if 'å­¦å¹´' in features_df.columns:
            features_df['å­¦å¹´_æ¨™æº–åŒ–'] = (features_df['å­¦å¹´'] - features_df['å­¦å¹´'].mean()) / features_df['å­¦å¹´'].std()
        
        if 'æ–™é‡‘ï¼ˆå…ƒï¼‰' in features_df.columns:
            features_df['æ–™é‡‘_æ¨™æº–åŒ–'] = (features_df['æ–™é‡‘ï¼ˆå…ƒï¼‰'] - features_df['æ–™é‡‘ï¼ˆå…ƒï¼‰'].mean()) / features_df['æ–™é‡‘ï¼ˆå…ƒï¼‰'].std()
        
        self.feature_names = features_df.columns.tolist()
        print(f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len(self.feature_names)}å€‹ã®ç‰¹å¾´é‡")
        print(f"ç‰¹å¾´é‡ä¸€è¦§: {self.feature_names}")
        
        return features_df
    
    def create_target_variables(self):
        """ç›®çš„å¤‰æ•°ã®ä½œæˆ"""
        print("ğŸ¯ ç›®çš„å¤‰æ•°ã‚’ä½œæˆä¸­...")
        
        targets = {}
        
        # 1. å‚åŠ ç¢ºç‡ï¼ˆç–‘ä¼¼ï¼‰: å­¦å¹´ã¨æ–™é‡‘ã«åŸºã¥ã
        # é«˜å­¦å¹´ã»ã©å‚åŠ ç¢ºç‡ãŒé«˜ãã€æ–™é‡‘ãŒé«˜ã™ãã‚‹ã¨å‚åŠ ç¢ºç‡ãŒä¸‹ãŒã‚‹
        grade_factor = self.df['å­¦å¹´'].fillna(0) / 4.0  # 0-1ã«æ­£è¦åŒ–
        cost_factor = 1 - (self.df['æ–™é‡‘ï¼ˆå…ƒï¼‰'].fillna(0) / 1000.0).clip(0, 1)  # æ–™é‡‘ãŒé«˜ã„ã»ã©å‚åŠ ç¢ºç‡ä½ä¸‹
        
        # ãƒã‚¤ã‚ºã‚’åŠ ãˆã¦ãƒªã‚¢ãƒ«ãªåˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹
        noise = np.random.normal(0, 0.1, len(self.df))
        participation_prob = (grade_factor * 0.6 + cost_factor * 0.4 + noise).clip(0, 1)
        
        targets['participation_prob'] = participation_prob
        targets['participation_binary'] = (participation_prob > 0.5).astype(int)
        
        # 2. ã‚¤ãƒ™ãƒ³ãƒˆæº€è¶³åº¦ï¼ˆç–‘ä¼¼ï¼‰: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã¨æ–™é‡‘ã®é–¢ä¿‚
        satisfaction_base = np.random.normal(0.7, 0.2, len(self.df))
        # æ–™é‡‘ãŒé©åˆ‡ãªç¯„å›²ã«ã‚ã‚‹ã¨æº€è¶³åº¦ãŒé«˜ã„
        cost_optimal = ((self.df['æ–™é‡‘ï¼ˆå…ƒï¼‰'].fillna(0) >= 200) & 
                       (self.df['æ–™é‡‘ï¼ˆå…ƒï¼‰'].fillna(0) <= 800)).astype(int)
        satisfaction = satisfaction_base + cost_optimal * 0.2
        targets['satisfaction'] = satisfaction.clip(0, 1)
        
        print(f"âœ… ç›®çš„å¤‰æ•°ä½œæˆå®Œäº†")
        print(f"å‚åŠ ç¢ºç‡ã®åˆ†å¸ƒ: å¹³å‡={participation_prob.mean():.3f}, æ¨™æº–åå·®={participation_prob.std():.3f}")
        print(f"å‚åŠ ç¢ºç‡>0.5ã®å‰²åˆ: {(participation_prob > 0.5).mean():.3f}")
        
        return targets
    
    def train_models(self, X, y_dict):
        """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_dict['participation_binary'], test_size=0.2, random_state=42, stratify=y_dict['participation_binary']
        )
        
        # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # 1. Random Forest Classifier
        print("  ğŸ“Š Random Forest Classifierã‚’è¨“ç·´ä¸­...")
        rf_clf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        rf_clf.fit(X_train_scaled, y_train)
        
        # 2. Random Forest Regressor (å‚åŠ ç¢ºç‡äºˆæ¸¬)
        print("  ğŸ“Š Random Forest Regressorã‚’è¨“ç·´ä¸­...")
        y_reg = y_dict['participation_prob']
        X_train_reg, X_test_reg = train_test_split(X, test_size=0.2, random_state=42)
        y_train_reg, y_test_reg = train_test_split(y_reg, test_size=0.2, random_state=42)
        
        X_train_reg_scaled = self.scaler.fit_transform(X_train_reg)
        X_test_reg_scaled = self.scaler.transform(X_test_reg)
        
        rf_reg = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        rf_reg.fit(X_train_reg_scaled, y_train_reg)
        
        # 3. Gradient Boosting Classifier
        print("  ğŸš€ Gradient Boosting Classifierã‚’è¨“ç·´ä¸­...")
        gb_clf = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        gb_clf.fit(X_train_scaled, y_train)
        
        # 4. Gradient Boosting Regressor
        print("  ğŸš€ Gradient Boosting Regressorã‚’è¨“ç·´ä¸­...")
        gb_reg = GradientBoostingRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        gb_reg.fit(X_train_reg_scaled, y_train_reg)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self.models = {
            'rf_classifier': rf_clf,
            'rf_regressor': rf_reg,
            'gb_classifier': gb_clf,
            'gb_regressor': gb_reg
        }
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ä¿å­˜
        self.feature_importance = {
            'rf_classifier': rf_clf.feature_importances_,
            'rf_regressor': rf_reg.feature_importances_,
            'gb_classifier': gb_clf.feature_importances_,
            'gb_regressor': gb_reg.feature_importances_
        }
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        self.evaluate_models(X_test_scaled, X_test_reg_scaled, y_test, y_test_reg)
        
        return X_test_scaled, X_test_reg_scaled, y_test, y_test_reg
    
    def evaluate_models(self, X_test_clf, X_test_reg, y_test_clf, y_test_reg):
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        print("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœ:")
        
        # åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        print("\n--- åˆ†é¡ãƒ¢ãƒ‡ãƒ« (å‚åŠ /ä¸å‚åŠ ) ---")
        for name, model in [('Random Forest', self.models['rf_classifier']), 
                           ('Gradient Boosting', self.models['gb_classifier'])]:
            y_pred = model.predict(X_test_clf)
            accuracy = accuracy_score(y_test_clf, y_pred)
            print(f"{name}: ç²¾åº¦ = {accuracy:.3f}")
        
        # å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        print("\n--- å›å¸°ãƒ¢ãƒ‡ãƒ« (å‚åŠ ç¢ºç‡) ---")
        for name, model in [('Random Forest', self.models['rf_regressor']), 
                           ('Gradient Boosting', self.models['gb_regressor'])]:
            y_pred = model.predict(X_test_reg)
            mse = mean_squared_error(y_test_reg, y_pred)
            print(f"{name}: RMSE = {np.sqrt(mse):.3f}")
    
    def visualize_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–"""
        print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–ä¸­...")
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        fig.suptitle('ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ', fontsize=16)
        
        models_info = [
            ('rf_classifier', 'Random Forest Classifier', axes[0,0]),
            ('rf_regressor', 'Random Forest Regressor', axes[0,1]),
            ('gb_classifier', 'Gradient Boosting Classifier', axes[1,0]),
            ('gb_regressor', 'Gradient Boosting Regressor', axes[1,1])
        ]
        
        for model_key, model_name, ax in models_info:
            if model_key in self.feature_importance:
                importance = self.feature_importance[model_key]
                feature_names = self.feature_names
                
                # é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆ
                indices = np.argsort(importance)[::-1]
                top_features = min(15, len(feature_names))  # ä¸Šä½15ç‰¹å¾´é‡
                
                ax.barh(range(top_features), importance[indices][:top_features])
                ax.set_yticks(range(top_features))
                ax.set_yticklabels([feature_names[i] for i in indices[:top_features]])
                ax.set_xlabel('é‡è¦åº¦')
                ax.set_title(f'{model_name}\nç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½{top_features})')
                ax.invert_yaxis()
        
        plt.tight_layout()
        plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')
        print("âœ… ç‰¹å¾´é‡é‡è¦åº¦ã‚’ 'feature_importance_analysis.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        plt.close()
    
    def predict_new_participants(self, sample_size=10):
        """æ–°ã—ã„å‚åŠ è€…ã®äºˆæ¸¬ä¾‹"""
        print(f"\nğŸ”® æ–°ã—ã„å‚åŠ è€…ã®äºˆæ¸¬ä¾‹ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size})")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        np.random.seed(42)
        sample_data = self.generate_sample_participants(sample_size)
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
        sample_features = self.feature_engineering_sample(sample_data)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = {}
        
        for model_name, model in self.models.items():
            if 'classifier' in model_name:
                pred = model.predict(sample_features)
                prob = model.predict_proba(sample_features)[:, 1] if hasattr(model, 'predict_proba') else pred
                predictions[model_name] = {
                    'prediction': pred,
                    'probability': prob
                }
            else:
                pred = model.predict(sample_features)
                predictions[model_name] = {
                    'prediction': pred
                }
        
        # çµæœã®è¡¨ç¤º
        print("\näºˆæ¸¬çµæœ:")
        for i in range(sample_size):
            print(f"\nå‚åŠ è€…{i+1}:")
            print(f"  å±æ€§: {sample_data.iloc[i]['æ€§åˆ¥']}, {sample_data.iloc[i]['å­¦å¹´']}å¹´, {sample_data.iloc[i]['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼']}")
            
            for model_name, pred_data in predictions.items():
                if 'probability' in pred_data:
                    print(f"  {model_name}: å‚åŠ ç¢ºç‡ = {pred_data['probability'][i]:.3f}")
                else:
                    print(f"  {model_name}: å‚åŠ ç¢ºç‡ = {pred_data['prediction'][i]:.3f}")
    
    def generate_sample_participants(self, n):
        """ã‚µãƒ³ãƒ—ãƒ«å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        sample_data = []
        
        genders = ['ç”·æ€§', 'å¥³æ€§']
        grades = [1, 2, 3, 4]
        majors = ['æƒ…å ±', 'äººæ–‡', 'å•†å­¦', 'å›½éš›', 'ç†å­¦', 'å·¥å­¦', 'åŒ»å­¦', 'æ•™è‚²']
        # å…ƒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹é–‹å‚¬åœ°ç‚¹ã®ã¿ä½¿ç”¨
        locations = ['å°ä¸­']  # å…ƒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹å€¤ã®ã¿
        costs = [200, 300, 500, 800, 1000]
        
        for i in range(n):
            participant = {
                'æ€§åˆ¥': np.random.choice(genders),
                'å­¦å¹´': np.random.choice(grades),
                'å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼': np.random.choice(majors),
                'é–‹å‚¬åœ°ç‚¹': np.random.choice(locations),
                'æ–™é‡‘ï¼ˆå…ƒï¼‰': np.random.choice(costs),
                'è¦æ¨¡ï¼ˆäººï¼‰': np.random.randint(20, 100)
            }
            sample_data.append(participant)
        
        return pd.DataFrame(sample_data)
    
    def feature_engineering_sample(self, sample_df):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        features_df = pd.DataFrame()
        
        # æ—¢å­˜ã®ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        for feature in ['æ€§åˆ¥', 'å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'é–‹å‚¬åœ°ç‚¹']:
            if feature in self.label_encoders:
                encoded_values = self.label_encoders[feature].transform(sample_df[feature].astype(str))
                features_df[f'{feature}_encoded'] = encoded_values
        
        # æ•°å€¤ç‰¹å¾´é‡
        features_df['å­¦å¹´'] = sample_df['å­¦å¹´']
        features_df['è¦æ¨¡ï¼ˆäººï¼‰'] = sample_df['è¦æ¨¡ï¼ˆäººï¼‰']
        features_df['æ–™é‡‘ï¼ˆå…ƒï¼‰'] = sample_df['æ–™é‡‘ï¼ˆå…ƒï¼‰']
        
        # çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡
        if 'å­¦å¹´' in features_df.columns and 'å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼_encoded' in features_df.columns:
            features_df['å­¦å¹´Ã—å­¦ç§‘'] = features_df['å­¦å¹´'] * features_df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼_encoded']
        
        # æ¨™æº–åŒ–
        features_scaled = self.scaler.transform(features_df)
        
        return features_scaled

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆ†æã‚’é–‹å§‹ã—ã¾ã™")
    
    # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    prediction_model = ParticipationPredictionModel('PXã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ è€…DB.csv')
    
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    X = prediction_model.feature_engineering()
    
    # ç›®çš„å¤‰æ•°ã®ä½œæˆ
    y_dict = prediction_model.create_target_variables()
    
    # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    prediction_model.train_models(X, y_dict)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
    prediction_model.visualize_feature_importance()
    
    # æ–°ã—ã„å‚åŠ è€…ã®äºˆæ¸¬ä¾‹ï¼ˆç°¡ç•¥åŒ–ï¼‰
    print("\nğŸ”® ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«è¨“ç·´ã•ã‚Œã€å‚åŠ äºˆæ¸¬ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
    print("   - åˆ†é¡ãƒ¢ãƒ‡ãƒ«: å‚åŠ /ä¸å‚åŠ ã‚’äºˆæ¸¬")
    print("   - å›å¸°ãƒ¢ãƒ‡ãƒ«: å‚åŠ ç¢ºç‡ã‚’äºˆæ¸¬")
    print("   - ç‰¹å¾´é‡é‡è¦åº¦: ã©ã®è¦å› ãŒå‚åŠ ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ã‚’åˆ†æ")
    
    print("\nâœ… å‚åŠ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print("- feature_importance_analysis.png: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")

if __name__ == "__main__":
    main()
