# pip install -q mecab-python3 unidic-lite
# pip install -q scikit-learn pandas numpy fugashi ipadic matplotlib seaborn

import pandas as pd
import numpy as np
import MeCab
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib
matplotlib.use('Agg')  # GUIã‚’ä½¿ã‚ãªã„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è¨­å®š
import matplotlib.pyplot as plt
import seaborn as sns

# MeCabã®Taggerã‚’åˆæœŸåŒ–
try:
    # è¾æ›¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    tagger = MeCab.Tagger()
except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ä»£æ›¿åˆæœŸåŒ–
    tagger = MeCab.Tagger("-r /etc/mecabrc -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd")

# ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªè¡¨ç¤ºè¨­å®š
import matplotlib.font_manager as fm

# åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
japanese_fonts = ['Hiragino Kaku Gothic ProN', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao Gothic']
available_fonts = [f.name for f in fm.fontManager.ttflist]

japanese_font = None
for font in japanese_fonts:
    if font in available_fonts:
        japanese_font = font
        break

if japanese_font:
    plt.rcParams['font.family'] = japanese_font
    print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{japanese_font}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
else:
    print("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
    plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False

file_name = "/Users/rin/Documents/Polycle/polycle_data-analysis/PXã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ è€…DB.csv"

# DataFrameã¸ã®èª­ã¿è¾¼ã¿
try:
    df = pd.read_csv(file_name, encoding='utf-8')
except UnicodeDecodeError:
    print("UTF-8ã§å¤±æ•—ã€‚Shift-JISã§å†è©¦è¡Œã—ã¾ã™ã€‚")
    df = pd.read_csv(file_name, encoding='shift_jis')

print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print(df.head(2))

# 'å­¦å¹´'â†’æ•°å€¤å‹ã«å¤‰æ›
df['å­¦å¹´'] = pd.to_numeric(df['å­¦å¹´'].astype(str).str.replace(r'[^\d]', '', regex=True), errors='coerce')

# å­¦å¹´ã®åˆ†å¸ƒã‚’ç¢ºèª
print("\n=== å­¦å¹´ã®åˆ†å¸ƒ ===")
print(df['å­¦å¹´'].value_counts().sort_index())
print(f"å­¦å¹´ã®ç¯„å›²: {df['å­¦å¹´'].min()} - {df['å­¦å¹´'].max()}")

# å­¦å¹´ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡
def categorize_grade(grade):
    """å­¦å¹´ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡"""
    if pd.isna(grade):
        return 'ãã®ä»–'
    
    grade = int(grade)
    
    if grade == 1:
        return '1å¹´ç”Ÿ'
    elif grade == 2:
        return '2å¹´ç”Ÿ'
    elif grade == 3:
        return '3å¹´ç”Ÿ'
    elif grade == 4:
        return '4å¹´ç”Ÿ'
    else:
        return 'é™¢ç”Ÿ/ç¤¾ä¼šäºº'

df['å­¦å¹´ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = df['å­¦å¹´'].apply(categorize_grade)

print("\n=== å­¦å¹´ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®åˆ†å¸ƒ ===")
print(df['å­¦å¹´ã‚«ãƒ†ã‚´ãƒªãƒ¼'].value_counts())

# ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–

def tokenize_text(text, tagger_instance):
    """MeCabã§å˜èªåˆ†å‰²ã—ã€æ„å‘³ã®ã‚ã‚‹åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿ã‚’æŠ½å‡º"""
    if pd.isna(text) or text is None: return ""
    node = tagger_instance.parseToNode(str(text))
    keywords = []

    while node:
        feature = node.feature.split(',')
        # å®‰å…¨ãƒã‚§ãƒƒã‚¯: ãƒªã‚¹ãƒˆã®é•·ã•ãŒ7æœªæº€ã®å ´åˆã¯åŸå½¢[6]ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
        if len(feature) >= 7 and feature[0] in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            base = feature[6] if feature[6] != '*' else node.surface
            if base not in ['ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã™ã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'çš„']:
                keywords.append(base)
        node = node.next
    return " ".join(keywords)

# tokenåŒ–
df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹_tokens'] = df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹'].apply(lambda x: tokenize_text(x, tagger))

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
tfidf = TfidfVectorizer(min_df=2)
tfidf_matrix = tfidf.fit_transform(df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹_tokens']).toarray()
tfidf_cols = [f'tfidf_{w}' for w in tfidf.get_feature_names_out()]
tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_cols, index=df.index)

# å…ƒã®DataFrameã¨çµåˆ
df = pd.concat([df, tfidf_df], axis=1)

print("âœ… TF-IDFãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚")

# æ‰‹å‹•ã§é¸æŠã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨è‡ªå‹•æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›
manual_keywords_japanese = ['ã‚·ãƒ¥ã‚¦ã‚«ãƒ„', 'ã‚ªã‚ªãƒ‹ãƒ³ã‚ºã‚¦', 'ã‚³ã‚¦ãƒªãƒ¥ã‚¦', 'ãƒ¯ã‚¤ãƒ¯ã‚¤', 'ã‚¸ãƒ§ã‚¦ãƒ›ã‚¦','OBOG', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³']

words_to_exclude = ['ãƒ‡ã‚­ãƒ«', 'ãƒ†ã‚¤ã‚­ãƒ§ã‚¦']

# è‡ªå‹•æŠ½å‡ºã•ã‚ŒãŸä¸Šä½10å˜èªã‚’å–å¾—
tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]
top_auto_keywords = [
    col.replace('tfidf_', '')
    for col in df[tfidf_cols].sum().sort_values(ascending=False).head(10).index.tolist()
]

# è‡ªå‹•æŠ½å‡ºå˜èªã¨æ‰‹å‹•å˜èªã‚’çµ±åˆ
all_target_words = list(set(top_auto_keywords + manual_keywords_japanese))

# é™¤å¤–ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
final_keywords_to_use = [
    word for word in all_target_words
    if word not in words_to_exclude
]

final_keywords_to_use = [
    f'tfidf_{word}' for word in final_keywords_to_use
    if f'tfidf_{word}' in df.columns
]

print("\n--- ğŸ’¡ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®æœ€çµ‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ---")
print(f"ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(final_keywords_to_use)} å€‹")
print(final_keywords_to_use)

# å­¦å¹´åˆ¥ã®TF-IDFåˆ†æ
analysis_target = 'å­¦å¹´ã‚«ãƒ†ã‚´ãƒªãƒ¼'

# é›†è¨ˆã«ä½¿ç”¨ã™ã‚‹TF-IDFåˆ—åã®å®šç¾©
keywords_to_use_for_agg = [col for col in final_keywords_to_use if col in df.columns]

if not keywords_to_use_for_agg:
    print("Error: None of the specified keywords were found as columns in the DataFrame. Stopping aggregation.")
    pivot_data_grade = pd.DataFrame()
else:
    print(f"âœ… é›†è¨ˆã«ä½¿ç”¨ã™ã‚‹TF-IDFåˆ—: {keywords_to_use_for_agg}")

    # å­¦å¹´åˆ¥ã«é›†è¨ˆ
    tfidf_agg = df.groupby(analysis_target)[keywords_to_use_for_agg].mean()

    # å‚åŠ è€…æ•°ã‚’è¨ˆç®—
    if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns:
        count_agg = df.groupby(analysis_target)['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].size().to_frame(name='participant_count')
    else:
        print("Error: 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' column not found in the DataFrame. Cannot calculate participant counts.")
        count_agg = pd.DataFrame(index=df[analysis_target].unique())

    # é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    if not count_agg.empty:
        pivot_data_grade = tfidf_agg.join(count_agg, how='left')
    else:
        pivot_data_grade = tfidf_agg

    # TF-IDFã®NaNã‚’0ã«ç½®æ›
    pivot_data_grade = pivot_data_grade.fillna(0)

    # å‚åŠ è€…2åä»¥ä¸Šã®å­¦å¹´ã«çµã‚‹
    if 'participant_count' in pivot_data_grade.columns:
        pivot_data_grade = pivot_data_grade[pivot_data_grade['participant_count'] > 1].drop(columns=['participant_count']).reset_index()
    else:
        print("Warning: 'participant_count' column not available for filtering. Proceeding without filtering by participant count.")
        pivot_data_grade = pivot_data_grade.reset_index()

    print("âœ… å­¦å¹´åˆ¥ãƒ‡ãƒ¼ã‚¿é›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# åˆ—åã®ãƒªãƒãƒ¼ãƒ ï¼ˆç•ªå·ã«ç½®ãæ›ãˆï¼‰
tfidf_column_names = [col for col in pivot_data_grade.columns if col.startswith('tfidf_')]
num_keywords = len(tfidf_column_names)

number_labels = [str(i) for i in range(num_keywords)]
rename_map_numbers = dict(zip(tfidf_column_names, number_labels))

pivot_data_grade_final = pivot_data_grade.rename(columns=rename_map_numbers)

print("âœ… ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã¨ç•ªå·ã¸ã®ãƒªãƒãƒ¼ãƒ ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®å¯è¦–åŒ–
plt.figure(figsize=(16, max(6, len(pivot_data_grade_final) * 0.6)))

if analysis_target in pivot_data_grade_final.columns and not pivot_data_grade_final.empty:
    heatmap_data = pivot_data_grade_final.set_index(analysis_target)

    # æ¨ªè»¸ã®é †åºã‚’é€£ç•ªã®é †ã«å›ºå®š
    final_number_columns = number_labels
    heatmap_data = heatmap_data[final_number_columns]

    # ãƒ‡ãƒ¼ã‚¿ã‚’è»¢ç½®ã—ã¦ç¸¦è»¸ã¨æ¨ªè»¸ã‚’é€†ã«ã™ã‚‹
    heatmap_data_transposed = heatmap_data.T
    
    sns.heatmap(
        heatmap_data_transposed,
        cmap='YlGnBu',
        annot=True,
        fmt=".3f",
        linewidths=.5,
        linecolor='lightgray',
        annot_kws={"fontsize": 10}
    )

    plt.title('Grade Level Event Interest Heatmap (Keyword Index)', fontsize=16)
    plt.ylabel('Keyword Index (0, 1, 2, 3, ...)', fontsize=12)
    plt.xlabel('Grade Level', fontsize=12)
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # ã‚°ãƒ©ãƒ•ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_filename = 'grade/grade_event_heatmap.png'
    plt.savefig(output_filename, dpi=300, bbox_inches='tight')
    print(f"âœ… å­¦å¹´åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ '{output_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    # GUIã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‹ã‹ãšã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
    plt.close()

else:
    print("Warning: Final data for heatmap is empty or missing the index column. Cannot plot.")

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã®å‡ºåŠ›
print("\n--- ğŸ’¡ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ ---")
japanese_keywords = [kw.replace('tfidf_', '') for kw in final_keywords_to_use]
keyword_table = pd.DataFrame({
    'Index': number_labels,
    'Keyword (Japanese)': japanese_keywords
})
print(keyword_table)

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
keyword_table.to_csv('grade/grade_heatmap_keywords.txt', sep='\t', index=False)
print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã‚’ 'grade_heatmap_keywords.txt' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

print("\n=== åˆ†æå®Œäº† ===")
print(f"åˆ†æå¯¾è±¡å­¦å¹´: {sorted(df['å­¦å¹´ã‚«ãƒ†ã‚´ãƒªãƒ¼'].unique())}")
print(f"ä½¿ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(final_keywords_to_use)}")
print(f"å‚åŠ è€…ç·æ•°: {len(df)}")
