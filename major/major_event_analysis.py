# pip install -q mecab-python3 unidic-lite
# pip install -q scikit-learn pandas numpy fugashi ipadic matplotlib seaborn

import pandas as pd
import numpy as np
import MeCab
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib
matplotlib.use('Agg')  # GUIã‚’ä½¿ã‚ãªã„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è¨­å®š
import matplotlib.pyplot as plt
import seaborn as sns

# MeCabã®Taggerã‚’åˆæœŸåŒ–
try:
    # è¾æ›¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    tagger = MeCab.Tagger()
except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ä»£æ›¿åˆæœŸåŒ–
    tagger = MeCab.Tagger("-r /etc/mecabrc -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd")

# ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªè¡¨ç¤ºè¨­å®š
import matplotlib.font_manager as fm

# åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
japanese_fonts = ['Hiragino Kaku Gothic ProN', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao Gothic']
available_fonts = [f.name for f in fm.fontManager.ttflist]

japanese_font = None
for font in japanese_fonts:
    if font in available_fonts:
        japanese_font = font
        break

if japanese_font:
    plt.rcParams['font.family'] = japanese_font
    print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{japanese_font}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
else:
    print("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
    plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False

file_name = "/Users/rin/Documents/Polycle/polycle_data-analysis/PXã‚¤ãƒ™ãƒ³ãƒˆå‚åŠ è€…DB.csv"

# DataFrameã¸ã®èª­ã¿è¾¼ã¿
try:
    df = pd.read_csv(file_name, encoding='utf-8')
except UnicodeDecodeError:
    print("UTF-8ã§å¤±æ•—ã€‚Shift-JISã§å†è©¦è¡Œã—ã¾ã™ã€‚")
    df = pd.read_csv(file_name, encoding='shift_jis')

print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print(df.head(2))

# 'å­¦å¹´'â†’æ•°å€¤å‹
df['å­¦å¹´'] = pd.to_numeric(df['å­¦å¹´'].astype(str).str.replace(r'[^\d]', '', regex=True), errors='coerce')

# ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–

def tokenize_text(text, tagger_instance):
    """MeCabã§å˜èªåˆ†å‰²ã—ã€æ„å‘³ã®ã‚ã‚‹åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿ã‚’æŠ½å‡º"""
    if pd.isna(text) or text is None: return ""
    node = tagger_instance.parseToNode(str(text))
    keywords = []

    while node:
        feature = node.feature.split(',')
        # å®‰å…¨ãƒã‚§ãƒƒã‚¯: ãƒªã‚¹ãƒˆã®é•·ã•ãŒ7æœªæº€ã®å ´åˆã¯åŸå½¢[6]ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
        if len(feature) >= 7 and feature[0] in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            base = feature[6] if feature[6] != '*' else node.surface
            if base not in ['ã“ã¨', 'ã‚‚ã®', 'ãã‚Œ', 'ã™ã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'çš„']:
                keywords.append(base)
        node = node.next
    return " ".join(keywords)

# tokenåŒ–
df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹_tokens'] = df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹'].apply(lambda x: tokenize_text(x, tagger))

# TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
tfidf = TfidfVectorizer(min_df=2)
tfidf_matrix = tfidf.fit_transform(df['ã‚¤ãƒ™ãƒ³ãƒˆå†…å®¹_tokens']).toarray()
tfidf_cols = [f'tfidf_{w}' for w in tfidf.get_feature_names_out()]
tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_cols, index=df.index)

# å…ƒã®DataFrameã¨çµåˆ
df = pd.concat([df, tfidf_df], axis=1)

# --- 3. æ™‚é–“å¸¯ã®ç‰¹å¾´é‡ï¼ˆé€±æœ«ãƒ•ãƒ©ã‚°ï¼‰---
df['é€±æœ«ãƒ•ãƒ©ã‚°'] = pd.to_datetime(df['æ—¥ã«ã¡'], errors='coerce').dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)

print("TF-IDFãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚")

def categorize_major(major_name):
    """
    å­¦ç§‘åã‚’æŒ‡å®šã•ã‚ŒãŸ11ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹é–¢æ•°
    """
    if pd.isna(major_name):
        return 'ãã®ä»–'

    name = str(major_name).upper()

    if 'æƒ…å ±' in name or 'è³‡è¨Š' in name or 'ç³»çµ±' in name or 'CS' in name or 'è³‡' in name:
        return 'æƒ…å ±'
    if 'åŒ»å­¦' in name or 'é†«' in name or 'è–¬å­¦' in name or 'ç‡Ÿé¤Š' in name:
        return 'åŒ»å­¦'
    if 'å·¥å­¦' in name or 'å·¥ç¨‹' in name or 'é›»æ©Ÿ' in name or 'æ©Ÿæ¢°' in name or 'ã‚·ã‚¹ãƒ†ãƒ ' in name or 'å·¥å­¸' in name or 'åŠå°ä½“' in name:
        return 'å·¥å­¦'
    if 'ç†å­¦' in name or 'ç§‘å­¸' in name or 'ç‰©ç†' in name or 'æ•¸å­¸' in name or 'åŒ–å­¸' in name:
        return 'ç†å­¦'
    if 'å»ºç¯‰' in name:
        return 'å»ºç¯‰'

    if 'è²¿æ˜“' in name or 'åœ‹éš›' in name or 'åœ‹' in name or 'å›½éš›' in name or 'å¤–äº¤' in name or 'Global' in name:
        return 'å›½éš›'
    if 'è¨€èª' in name or 'èªå­¸' in name or 'æ—¥æ–‡' in name or 'è‹±æ–‡' in name or 'è‹±èª' in name or 'å¤–æ–‡' in name or 'ä¸­æ–‡' in name or 'BIBA' in name or 'æ–‡å­¸' in name in name or 'äººæ–‡' in name in name or 'ç¤¾ä¼šå­¦' in name or 'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³' in name or 'å‚³æ’­' in name or 'æ–°è' in name or 'å»£æ’­' in name or 'æ–‡å­¦' in name:
        return 'äººæ–‡'
    if 'çµŒæ¸ˆ' in name or 'ä¼ç®¡' in name or 'å•†å­¸' in name or 'Business' in name or 'ç¶“æ¿Ÿ' in name or 'çµŒå–¶' in name or 'ç®¡ç†' in name or 'é‡‘è' in name or 'æœƒè¨ˆ' in name or 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°' in name or 'è¡ŒéŠ·' in name or 'é‡‘' in name or 'æ”¿æ²»' in name or 'Management' in name:
        return 'å•†å­¦'
    if 'æ•™è‚²' in name or 'å¸«è³‡' in name or 'æ•™è‚²' in name or 'æ•™é¤Š' in name:
        return 'æ•™è‚²'
    if 'è¦³å…‰' in name or 'è§€å…‰' in name:
        return 'è¦³å…‰'
    if 'é‹å‹•' in name:
        return 'é‹å‹•'
    if 'è¨­è¨ˆ' in name:
        return 'ãƒ‡ã‚¶ã‚¤ãƒ³'

    return 'ãã®ä»–'

df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = df['å­¦ç§‘'].apply(categorize_major)

print("âœ… å­¦ç§‘åã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print(df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'].value_counts())
print(df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'])

other_majors = df[df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == 'ãã®ä»–']['å­¦ç§‘'].unique()

print("\n--- ãã®ä»– ---")
if len(other_majors) > 0:
    for major in other_majors:
        print(f"- {major}")


# æ—¥æœ¬èªã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è‹±èªã‚«ãƒ†ã‚´ãƒªã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ï¼ˆmatlibã§è±†è…å›é¿ã™ã‚‹ãŸã‚ï¼‰
category_mapping = {
    'æƒ…å ±': 'IT',
    'å•†å­¦': 'Business',
    'äººæ–‡': 'Humanities',
    'å›½éš›': 'International',
    'ç†å­¦': 'Science',
    'æ•™è‚²': 'Education',
    'å·¥å­¦': 'Engineering',
    'åŒ»å­¦': 'Medicine',
    'å»ºç¯‰': 'Architecture',
    'ãƒ‡ã‚¶ã‚¤ãƒ³': 'Design',
    'é‹å‹•': 'Exercise',
    'è¦³å…‰': 'Tourism',
    'ãã®ä»–': 'Other'
}

# æ–°ã—ã„è‹±èªã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’ä½œæˆ
df['Major_Category_EN'] = df['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼'].map(category_mapping).fillna('Other')

# ãƒãƒƒãƒ”ãƒ³ã‚°ãŒæ¼ã‚ŒãŸå ´åˆï¼ˆç†è«–çš„ã«ã¯ãªã„ã¯ãšï¼‰ã«å‚™ãˆã¦ç¢ºèª
df.loc[~df['Major_Category_EN'].isin(category_mapping.values()), 'Major_Category_EN'] = 'Other'

print("âœ… æ–°ã—ã„åˆ— 'Major_Category_EN' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
print(df[['å­¦ç§‘ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'Major_Category_EN']].head())

import matplotlib.pyplot as plt
import seaborn as sns

analysis_target = 'Major_Category_EN'


manual_keywords_japanese = ['ã‚·ãƒ¥ã‚¦ã‚«ãƒ„', 'ã‚ªã‚ªãƒ‹ãƒ³ã‚ºã‚¦', 'ã‚³ã‚¦ãƒªãƒ¥ã‚¦', 'ãƒ¯ã‚¤ãƒ¯ã‚¤', 'ã‚¸ãƒ§ã‚¦ãƒ›ã‚¦','OBOG', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³']


# è‡ªå‹•æŠ½å‡ºã•ã‚ŒãŸä¸Šä½10å˜èªã‚’å–å¾—
tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]
top_auto_keywords = [
    col.replace('tfidf_', '')
    for col in df[tfidf_cols].sum().sort_values(ascending=False).head(10).index.tolist()
]

# 3. è‡ªå‹•æŠ½å‡ºå˜èªã¨æ‰‹å‹•å˜èªã‚’çµ±åˆ
all_target_words = list(set(top_auto_keywords + manual_keywords_japanese))

all_target_words = [
    f'tfidf_{word}' for word in all_target_words
    if f'tfidf_{word}' in df.columns
]

print("--- ğŸ’¡ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®æœ€çµ‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ---")
print(f"ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(all_target_words)} å€‹")
print(all_target_words)


# 1. ã€ä¿®æ­£ã€‘é›†è¨ˆã«ä½¿ç”¨ã™ã‚‹TF-IDFåˆ—åã®å®šç¾©
keywords_to_use_for_agg = [col for col in all_target_words if col in df.columns]

if not keywords_to_use_for_agg:
    print("Error: None of the specified keywords were found as columns in the DataFrame. Stopping aggregation.")

    pivot_data_en = pd.DataFrame()
else:
    print(f"âœ… é›†è¨ˆã«ä½¿ç”¨ã™ã‚‹TF-IDFåˆ—: {keywords_to_use_for_agg}")



analysis_target = 'Major_Category_EN'

# 1. ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã¨å‰å‡¦ç†

# Calculate TF-IDF means separately
tfidf_agg = df.groupby(analysis_target)[all_target_words].mean()

# Calculate mail counts separately
if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns:
    count_agg = df.groupby(analysis_target)['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].size().to_frame(name='mail_count')
else:
    print("Error: 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' column not found. Cannot calculate mail counts.")
    count_agg = pd.DataFrame(index=df[analysis_target].unique(), columns=['mail_count'])

# Merge the two aggregated DataFrames
pivot_data_en = tfidf_agg.join(count_agg, how='left')

# TF-IDFã®NaNï¼ˆç„¡é–¢å¿ƒï¼‰ã‚’0ã«ç½®æ›
pivot_data_en = pivot_data_en.fillna(0)

# å‚åŠ è€…3åä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«çµã‚‹
if 'mail_count' in pivot_data_en.columns:
    pivot_data_en = pivot_data_en[pivot_data_en['mail_count'] > 2]
    pivot_data_en = pivot_data_en.drop(columns=['mail_count']).reset_index()
else:
    print("Warning: 'mail_count' column not available for filtering. Proceeding without filtering.")
    pivot_data_en = pivot_data_en.reset_index()


# 2. åˆ—åã®ãƒªãƒãƒ¼ãƒ ï¼ˆç•ªå·ã«ç½®ãæ›ãˆï¼‰
# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«ä½¿ç”¨ã•ã‚Œã‚‹æœ€çµ‚çš„ãªåˆ—åï¼ˆTF-IDFåˆ—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
tfidf_column_names = [col for col in pivot_data_en.columns if col.startswith('tfidf_')]
num_keywords = len(tfidf_column_names)

number_labels = [str(i) for i in range(num_keywords)]
rename_map_numbers = dict(zip(tfidf_column_names, number_labels))

pivot_data_top_final = pivot_data_en.rename(columns=rename_map_numbers)

print("âœ… ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã¨ç•ªå·ã¸ã®ãƒªãƒãƒ¼ãƒ ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


# 3. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®å¯è¦–åŒ–

plt.figure(figsize=(16, max(8, len(pivot_data_top_final) * 0.8)))

if analysis_target in pivot_data_top_final.columns and not pivot_data_top_final.empty:
    heatmap_data = pivot_data_top_final.set_index(analysis_target)

    final_number_columns = number_labels
    heatmap_data = heatmap_data[final_number_columns]

    # ãƒ‡ãƒ¼ã‚¿ã‚’è»¢ç½®ã—ã¦ç¸¦è»¸ã¨æ¨ªè»¸ã‚’é€†ã«ã™ã‚‹
    heatmap_data_transposed = heatmap_data.T
    
    sns.heatmap(
        heatmap_data_transposed,
        cmap='YlGnBu',
        annot=True,
        fmt=".3f",
        linewidths=.5,
        linecolor='lightgray',
        annot_kws={"fontsize": 10}
    )

    plt.title('Major Category Event Interest Heatmap (Keyword Index)')
    plt.ylabel('Keyword Index (0, 1, 2, 3, ...)')
    plt.xlabel('Major Category')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # ã‚°ãƒ©ãƒ•ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_filename = 'major/major_event_heatmap.png'
    plt.savefig(output_filename, dpi=300, bbox_inches='tight')
    print(f"âœ… ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ '{output_filename}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    plt.close()

else:
    print("Warning: Final data for heatmap is empty or missing the index column. Cannot plot.")



# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã®å‡ºåŠ›
print("\n--- ğŸ’¡ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ ---")
japanese_keywords = [kw.replace('tfidf_', '') for kw in all_target_words]
keyword_table = pd.DataFrame({
    'Index': number_labels,
    'Keyword (Japanese)': japanese_keywords
})
print(keyword_table)

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
keyword_table.to_csv('major/major_event_heatmap_keywords.txt', sep='\t', index=False)
print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œè¡¨ã‚’ 'major_heatmap_keywords.txt' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")